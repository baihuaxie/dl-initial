{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Notes on torch.tensor operations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Find the maximum values and corresponding indices along axis=dim in a tensor \"\"\"\r\n",
    "\r\n",
    "x = torch.empty(5,3)\r\n",
    "values, indices = torch.max(x,dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" convert a tuple into torch.Size object \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "shape = (4, 5, 6)\r\n",
    "print(shape)\r\n",
    "shape = torch.Size(shape)\r\n",
    "print(shape)\r\n",
    "torch.Tensor(shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" convert a torch.tensor or torch.Size object into a tuple \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randn(5)\r\n",
    "tuple(x.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" reshape tensor \"\"\"\r\n",
    "x.reshape((1,) + tuple(x.shape))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" Converting between tensors and ndarrays \"\"\"\r\n",
    "\r\n",
    "# convert a tensor to a ndarray\r\n",
    "x = torch.randn(5,3)\r\n",
    "y = x.numpy()\r\n",
    "\r\n",
    "# conver a ndarray to a tensor\r\n",
    "z = torch.from_numpy(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" tensor slice while maintaining shape \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (1, 5, 16))\r\n",
    "print(x)\r\n",
    "x[:,0:2,:]\r\n",
    "# note that x[:,0,:] will reduce dimension; must use slicing [i:j] notation rather than indexing [i] notation"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" basic tensor slicing \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (3,4,5))\r\n",
    "print(x)\r\n",
    "### slice along dim=1; can omit rest of dimensions;\r\n",
    "print(x[:,:2])\r\n",
    "### equivalent;\r\n",
    "assert torch.all(x[:,:2] == x[:,:2,:])\r\n",
    "y = torch.randint(0, 10, (2,3,4))\r\n",
    "print(y)\r\n",
    "y[None, :, :].shape # adds a dummy dimension at dim=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" change tensor data type \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (20,))\r\n",
    "print(x.dtype)\r\n",
    "x = x.float()\r\n",
    "print(x.dtype)\r\n",
    "x = x.double()\r\n",
    "print(x.dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_note_:\n",
    "* if x.requires_grad=True, can not call numpy()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" use torch.tensor.item() to return single-element tensor as a python number \"\"\"\r\n",
    "\r\n",
    "x = torch.tensor([1.0])\r\n",
    "x.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" return size of a tensor \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randn(5,3)\r\n",
    "# return a torch.Size object of dimensions along each axis\r\n",
    "print(x.size())\r\n",
    "print(x.shape)\r\n",
    "# return number of axis\r\n",
    "print(x.dim())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" sum along a tensor dimension \"\"\"\r\n",
    "\r\n",
    "x = torch.randn(5,3,4)\r\n",
    "print(x)\r\n",
    "torch.sum(x,dim=1,keepdim=False)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" detach() \"\"\"\r\n",
    "\r\n",
    "x = torch.randn(5, 3, requires_grad=True)\r\n",
    "print(x)\r\n",
    "y = torch.sum(x * 2)\r\n",
    "print(x.requires_grad, y.requires_grad)\r\n",
    "\r\n",
    "# create z by detach() x from compute graph\r\n",
    "z = y.detach()\r\n",
    "print(y.requires_grad)          # x is un-modified\r\n",
    "print(z.requires_grad)          # z.requires_grad set to False\r\n",
    "\r\n",
    "# call backward() method on y, with requires_grad=True\r\n",
    "y.backward()\r\n",
    "print(x.grad)\r\n",
    "\r\n",
    "# z and y shares same storage, any change to z will update y\r\n",
    "print(y)\r\n",
    "print(z)\r\n",
    "z += 1\r\n",
    "print(z)\r\n",
    "print(y)                        # y is also updated to +1\r\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" apply element-wise transformations to tensor \"\"\"\r\n",
    "\r\n",
    "x = torch.randn(5, 3)\r\n",
    "print(x)\r\n",
    "\r\n",
    "# take exp\r\n",
    "x = torch.exp(x)\r\n",
    "print(x)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_note_:\r\n",
    "* Pytorch does not currently support custom element-wise lambda functions for tensor\r\n",
    "  * a solution maybe to convert to np.ndarrays first, or use a stack of built-in element-wise functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" conditional slicing \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.rand(32,10)                               # x: a 32x10 tensor (e.g., xent outputs where batch=32, num of classes = 10)\r\n",
    "labels = torch.randint(0,10,(32,))                  # labels: a 32x1 tensor of integers (e.g., each element is a correct label index)\r\n",
    "print(x)\r\n",
    "print(labels)\r\n",
    "\r\n",
    "# torch.max(tensor, dim) will return a tuple of two tensors (val_max, arg_max)\r\n",
    "max_val, indices = torch.max(x, dim=1)\r\n",
    "# tensor[condition] (for 1d tensor; for higher d, use slicing syntax like [:, condition]) slices the tensor if the condition is evaluated to be True element-wise\r\n",
    "torch.sum(max_val[indices == labels]).item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" masked slicing conditioned on another tensor \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.zeros(32,10)                               # x: a 32x10 tensor (e.g., xent outputs where batch=32, num of classes = 10)\r\n",
    "labels = torch.randint(0,10,(32,))                  # labels: a 32x1 tensor of integers (e.g., each element is a correct label index)\r\n",
    "\r\n",
    "# construct a 32x10 mask tensor obj, mask[i][j] = True if labels[i] == j -> use values in labels as indices\r\n",
    "# note that gather(), select() methods all broadcast indices along axis other than dim specified, so won't work here\r\n",
    "mask = torch.BoolTensor([[True if i == labels[j] else False for i in range(x.size()[1]) ] for j in range(x.size()[0])])\r\n",
    "torch.masked_select(x, mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_note_:\n",
    "* mask must be torch.BoolTensor type"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" check if two tensors are equal \"\"\"\r\n",
    "\r\n",
    "# torch.equal() returns True if all elements are equal\r\n",
    "x = torch.rand(3,3)\r\n",
    "y = torch.rand(3,3)\r\n",
    "\r\n",
    "# returns a final boolean with torch.equal()\r\n",
    "print(torch.equal(x,y))\r\n",
    "# returns a BoolTensor with torch.eq()\r\n",
    "print(torch.eq(x,y))\r\n",
    "# or\r\n",
    "print(torch.all(torch.eq(x,y)))\r\n",
    "\r\n",
    "z = x.clone()\r\n",
    "print(torch.equal(x,z))\r\n",
    "\r\n",
    "# to return True if no elements are equal, can do the following\r\n",
    "print((x != y).all())\r\n",
    "\r\n",
    "# to return True if at least some elements are not equal, can do the following\r\n",
    "print((x != y).any())"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" return all zero elements' indices in a tensor \"\"\"\r\n",
    "\r\n",
    "x = torch.rand(3,3)\r\n",
    "y = torch.rand(3,3)\r\n",
    "y[:,2] = x[:,2]\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "\r\n",
    "# return the non-zero element indices as a tuple\r\n",
    "tup = (x - y).nonzero(as_tuple=True)\r\n",
    "print(tup)\r\n",
    "# can directly use the returned tuple to access the tensor elements\r\n",
    "print(x[tup])\r\n",
    "\r\n",
    "# return the zero element indices as a tuple\r\n",
    "tup2 = ((x - y) == 0).nonzero(as_tuple=True)\r\n",
    "print(tup2)\r\n",
    "print(x[tup2])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.stack() vs torch.cat()\r\n",
    "* stack() a list of tensors along a new axis, output tensor would have an additional axis than input tensors\r\n",
    "* cat() a list of tensors along an existing axis, output tensor has equal # of axes as input tensors\r\n",
    "* stack() = unsqueeze() + cat()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.stack() + torch.transpose() + torch.flatten() \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "t = torch.rand(1, 9, 1, 1)\r\n",
    "print(t)\r\n",
    "print(t.shape)\r\n",
    "lst = torch.split(t, 3, dim=1)\r\n",
    "print(lst)\r\n",
    "a = torch.cat(lst, dim=1)\r\n",
    "print('break')\r\n",
    "print(a)\r\n",
    "print(a.shape)\r\n",
    "\r\n",
    "x = torch.stack(lst, dim=1)\r\n",
    "print(x)\r\n",
    "print(x.shape)\r\n",
    "\r\n",
    "y = torch.transpose(x, dim0=1, dim1=2)\r\n",
    "print(y)\r\n",
    "print(y.shape)\r\n",
    "\r\n",
    "z = torch.flatten(y, start_dim=1, end_dim=2)\r\n",
    "print(z)\r\n",
    "print(z.shape)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = torch.randint(0, 10, (12,))\r\n",
    "print(x)\r\n",
    "y = x.numpy()\r\n",
    "print(y)\r\n",
    "import numpy as np\r\n",
    "np.array_split(y, 12)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x1 = torch.rand(1,)\r\n",
    "x2 = torch.rand(1,)\r\n",
    "x3 = torch.rand(1,)\r\n",
    "lst = [x1, x2, x3]\r\n",
    "print(x1)\r\n",
    "print(x2)\r\n",
    "print(x3)\r\n",
    "torch.stack(lst, dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" keep in mind the difference between tensor([1]), tensor(1), tensor(1.)\"\"\"\r\n",
    "\r\n",
    "x1 = torch.tensor([1,2,3])\r\n",
    "x2 = torch.tensor([1])\r\n",
    "x3 = torch.tensor(1)\r\n",
    "print(x2)\r\n",
    "print(x3)\r\n",
    "print(x2.dim())\r\n",
    "print(x3.dim())\r\n",
    "# float\r\n",
    "x4 = torch.tensor(1.)\r\n",
    "print(x4)\r\n",
    "print(x4.shape)\r\n",
    "x5 = torch.tensor(1)\r\n",
    "print(x5)\r\n",
    "print(x5.reshape((1,)))\r\n",
    "x3.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "lst = [torch.tensor(1), torch.tensor(1), torch.tensor(1)]\r\n",
    "torch.cat(lst, dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" element-wise tensor multiplication by broadcasting \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (5, 3, 3))\r\n",
    "y = torch.randint(0, 10, (5, 1, 1))\r\n",
    "print(x)\r\n",
    "print(y)\r\n",
    "# can directly multiply two tensors, if one of the axes matches in dimensions\r\n",
    "x * y"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" \r\n",
    "torch.repeat_interleave(x, repeat, dim)\r\n",
    "\r\n",
    "- repeats every entry in tensor x by number=repeat for that dimension\r\n",
    "- repeat must have the same size as input along dim\r\n",
    "- dim is optional; if not specified, repeat must be integer\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\r\n",
    "print(x)\r\n",
    "\r\n",
    "# repeat all entries in all dimensions & return a flattened tensor\r\n",
    "x_1 = torch.repeat_interleave(x, 2)\r\n",
    "print(x_1)\r\n",
    "\r\n",
    "# repeat entry x[i] by repeat[i] along dim=0\r\n",
    "x_2 = torch.repeat_interleave(x, torch.tensor([1,2,3,4]), dim=0)\r\n",
    "print(x_2)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" create a random tensors for specified shape \"\"\"\r\n",
    "\r\n",
    "# integers; each entry within range [0, 9]; shape = (2,2,4)\r\n",
    "x = torch.randint(0, 10 ,(2, 2, 4))\r\n",
    "\r\n",
    "# random floats of shape torch.tensor([5,3,4]) sampled from N(0, 1)\r\n",
    "y_n = torch.randn(5, 3, 4)\r\n",
    "# random floats sampled from [0, 1] uniformly\r\n",
    "y_p = torch.rand(5, 3, 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" return data type in tensor \"\"\"\r\n",
    "import torch\r\n",
    "x = torch.randn(5, 3)\r\n",
    "len(x.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" \r\n",
    "enumerate a tensor \r\n",
    "\r\n",
    "- equivalent to split a tensor along dim=0\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "x = torch.randn(5, 3, 4)\r\n",
    "print(x)\r\n",
    "\r\n",
    "for count, matrix in enumerate(x):\r\n",
    "    print(count)\r\n",
    "    print(matrix)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "torch.bmm(input1, input2)\r\n",
    "\r\n",
    "- batch matrix-matrix product\r\n",
    "- input1=bxnxm, input2=bxmxp; returns bxnxp\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "torch.bmm(torch.ones(2, 1, 3), torch.ones(2, 3, 2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "torch.arange(start=0, stop, step=1)\r\n",
    "\r\n",
    "- generate a 1-D tensor of arithmetic sequence\r\n",
    "\r\n",
    "note:\r\n",
    "- start=0, step=1 as defaults, these two arguments are optional\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "# start=1, end=10, step=2\r\n",
    "x = torch.arange(1, 10, 2)\r\n",
    "print(x)\r\n",
    "# start=1, end=10, step=1\r\n",
    "y = torch.arange(10)\r\n",
    "print(y)\r\n",
    "z = torch.randint(0, 10, (2,3))\r\n",
    "print(z)\r\n",
    "print(torch.arange(z.shape[1]))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" product of all elements in tensor \"\"\"\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (2, 2))\r\n",
    "print(x)\r\n",
    "print(torch.prod(x, dim=0))\r\n",
    "print(torch.prod(x, dim=1))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.repeat \"\"\"\r\n",
    "import torch\r\n",
    "x = torch.randint(0, 10, (2, 2))\r\n",
    "print(x)\r\n",
    "x.repeat(2,3)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.clamp clips all elements in input tensor by a range \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (3, 4, 5))\r\n",
    "print(x)\r\n",
    "y = torch.clamp(x, min=3, max=6)\r\n",
    "print(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* convert from numpy.dtype to torch.dtype is problematic\n",
    "* see [this post](https://discuss.pytorch.org/t/converting-a-numpy-dtype-to-torch-dtype/52279)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.tensor vs torch.from_numpy() \"\"\"\r\n",
    "\r\n",
    "# from_numpy() expects a np.ndarray, so can not work for scalars\r\n",
    "# otherwise two method yields identical tensors?"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.BoolTensor requires a list to instantiate \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.BoolTensor([True for _ in range(10)])\r\n",
    "print(x)\r\n",
    "\r\n",
    "y = torch.BoolTensor((3,))\r\n",
    "print(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" use einops \"\"\"\r\n",
    "\r\n",
    "from einops import rearrange\r\n",
    "\r\n",
    "data, labels = batch\r\n",
    "print(data.shape)\r\n",
    "a = rearrange(data[0:1, :3], 'c h w -> h w c')\r\n",
    "print(a.shape)\r\n",
    "b = rearrange(a, 'h w c -> () c h w')\r\n",
    "print(b.shape)\r\n",
    "c = rearrange(b, 'b c h w -> b (c h w)')\r\n",
    "print(c.shape)\r\n",
    "d = rearrange(c, '() classes -> classes')\r\n",
    "print(d.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.unsqueeze() \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.tensor([1,2,3,4])\r\n",
    "print(x.shape)\r\n",
    "x1 = x.unsqueeze(0)\r\n",
    "print(x1)\r\n",
    "print(x1.shape)\r\n",
    "x2 = x.unsqueeze(1)\r\n",
    "print(x2)\r\n",
    "print(x2.shape)\r\n",
    "x3 = x.unsqueeze(-1)\r\n",
    "print(x3)\r\n",
    "print(x3.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" some special tensor methods \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.rand((2,2))\r\n",
    "print(x)\r\n",
    "\r\n",
    "### returns the sign of a tensor\r\n",
    "print(x.sign())\r\n",
    "### matrix transpose (dim <= 2)\r\n",
    "print(x.t())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.norm: now deprecated; use torch.linalg.norm instead \"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" check all elements in a tensor satisfies a condition or not \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (2,3,4))\r\n",
    "print(x)\r\n",
    "print(torch.all(x >= 0))\r\n",
    "\r\n",
    "y = torch.randn((2,3,4))\r\n",
    "print(y)\r\n",
    "print(torch.all(y >= 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" produce an identity matrix \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.ones(3)\r\n",
    "torch.diag(x)\r\n",
    "\r\n",
    "### a better way is to use torch.eye()\r\n",
    "torch.eye(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.unbind() \"\"\"\r\n",
    "\r\n",
    "# returns a tuple of tensors s/t the dimension of dim is removed;\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\r\n",
    "print(x)\r\n",
    "print(x.unbind(dim=1))\r\n",
    "\r\n",
    "y = torch.randint(0, 10, (2,3,4))\r\n",
    "print(y.shape)\r\n",
    "from einops import rearrange\r\n",
    "y1 = rearrange(y, '... (d j) -> ... d j', j=2)\r\n",
    "print(y1.shape)\r\n",
    "print(y)\r\n",
    "print(y1)\r\n",
    "z1, z2 = y1.unbind(dim=-1)\r\n",
    "print(z1)\r\n",
    "print(z2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" torch.expand \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (5,))\r\n",
    "print(x)\r\n",
    "print(x.shape)\r\n",
    "y = x.expand((2, 5))\r\n",
    "print(y)\r\n",
    "print(y.shape)\r\n",
    "\r\n",
    "### expand(1,-1) has the same effect of adding a dummy dimension at shape[0];\r\n",
    "z = x.expand((1, -1))\r\n",
    "print(z)\r\n",
    "print(z.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = (1,2,3,4)\r\n",
    "print(x)\r\n",
    "print(*x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "x = torch.tensor([[1,2],[3,4]]).numpy()\r\n",
    "y = torch.tensor([5,6,7]).numpy()\r\n",
    "\r\n",
    "z = np.multiply.outer(x, y)\r\n",
    "z = torch.from_numpy(z)\r\n",
    "print(z.shape)\r\n",
    "print(z)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (1, 2, 5, 10))\r\n",
    "y = torch.randint(0, 10, (1, 2, 3, 10))\r\n",
    "\r\n",
    "a = (2 ** 2)\r\n",
    "print(a)\r\n",
    "\r\n",
    "print((x * a))\r\n",
    "\r\n",
    "torch.einsum('...ik, ...jk -> ...ij', (x * 2), y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" tensor slicing: alternate odd / even positions. \"\"\"\r\n",
    "import torch\r\n",
    "from einops import rearrange\r\n",
    "\r\n",
    "x = torch.randint(0, 10, (4, 6))\r\n",
    "print(x)\r\n",
    "a = x[:, 0::2]      # even  \r\n",
    "b = x[:, 1::2]      # odd\r\n",
    "print(a)\r\n",
    "print(b)\r\n",
    "print(x.shape)\r\n",
    "print(a.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" several ways to interleave two tensors. \"\"\"\r\n",
    "import torch\r\n",
    "\r\n",
    "a = torch.ones([2, 3, 4])\r\n",
    "b = torch.zeros([2, 3, 4])\r\n",
    "print(a.shape)\r\n",
    "# interleave along dim = 1\r\n",
    "x = map(lambda t: torch.stack(t, dim=2),  zip(a.unbind(dim=1), b.unbind(dim=1)))\r\n",
    "print(torch.cat(list(x), dim=2))\r\n",
    "# interleave along dim = 2\r\n",
    "x = map(lambda t: torch.stack(t, dim=1),  zip(a.unbind(dim=2), b.unbind(dim=2)))\r\n",
    "print(torch.cat(list(x), dim=1))\r\n",
    "\r\n",
    "# there is probably no need to do unbind, can just stack directly then view;\r\n",
    "a = torch.ones([3, 4])\r\n",
    "b = torch.zeros([3, 4])\r\n",
    "print(torch.stack((a, b), dim=2).view(3, 8))\r\n",
    "print(torch.stack((a, b), dim=1).view(6, 4))\r\n",
    "\r\n",
    "# or simply use rearrange; note that to interleave must always put `t` in the last within the brackets, e.g., (w t), not (t w)\r\n",
    "from einops import rearrange\r\n",
    "a = torch.ones([3, 4])\r\n",
    "b = torch.zeros([3, 4])\r\n",
    "print(rearrange([a, b], 't h w -> h (w t)'))\r\n",
    "print(rearrange([a, b], 't h w -> (h t) w'))\r\n",
    "# how about more dimensions?\r\n",
    "a = torch.ones([2, 3, 4])\r\n",
    "b = torch.zeros([2, 3, 4])\r\n",
    "print(rearrange([a, b], 't c h w -> c h (w t)'))    # interleave along dim = 2, shape = [2,3,8]\r\n",
    "print(rearrange([a, b], 't c h w -> c (h t) w'))    # interleave along dim = 1, shape = [2,6,4]\r\n",
    "print(rearrange([a, b], 't c h w -> (c t) h w'))    # interleave along dim = 0, shape = [4,3,3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" concat two tensors along dim then interleave them along the same dimension. \"\"\"\r\n",
    "\r\n",
    "a = torch.ones([2, 3])\r\n",
    "b = torch.zeros([2, 3])\r\n",
    "torch.stack((a, b), dim=-1).view(2, 6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c04e14e757fcd5c931b20c230f107ce3be1b6ffeb36695f3d01a868d65a6b9cc"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}