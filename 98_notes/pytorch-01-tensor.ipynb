{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on torch.tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Find the maximum values and corresponding indices along axis=dim in a tensor \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.empty(5,3)\n",
    "values, indices = torch.max(x,dim=1)\n",
    "\n",
    "# if just skip `dim`, will get the max element in the entire tensor\n",
    "x = torch.randint(0, 9, (2, 3, 4))\n",
    "print(x)\n",
    "print(torch.max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" convert a tuple into torch.Size object \"\"\"\n",
    "import torch\n",
    "\n",
    "shape = (4, 5, 6)\n",
    "print(shape)\n",
    "shape = torch.Size(shape)\n",
    "print(shape)\n",
    "torch.Tensor(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" convert a torch.tensor or torch.Size object into a tuple \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randn(5)\n",
    "tuple(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" reshape tensor \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randn(5)\n",
    "print(x.shape)\n",
    "x1 = x.reshape((1,) + tuple(x.shape))\n",
    "print(x1.shape)\n",
    "\n",
    "y = torch.randn(2, 3, 4)\n",
    "print(y.shape)\n",
    "y1 = y.reshape(-1)          # flatten the tensor to 1d\n",
    "print(y1.shape)\n",
    "y2 = y.reshape(2, -1, 4)\n",
    "print(y2.shape)\n",
    "y3 = y.reshape(2, -1)       # -1 means to infer the dimension; it is usually used when I don't actually know the exact number or variable representing this dimension;\n",
    "print(y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Converting between tensors and ndarrays \"\"\"\n",
    "\n",
    "# convert a tensor to a ndarray\n",
    "x = torch.randn(5,3)\n",
    "y = x.numpy()\n",
    "\n",
    "# conver a ndarray to a tensor\n",
    "z = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tensor slice while maintaining shape \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (1, 5, 16))\n",
    "print(x)\n",
    "print(x[:,0:2,:])\n",
    "# note that x[:,0,:] will reduce dimension; must use slicing [i:j] notation rather than indexing [i] notation\n",
    "print(x[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" basic tensor slicing \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (3,4,5))\n",
    "print(x)\n",
    "### slice along dim=1; can omit rest of dimensions;\n",
    "print(x[:,:2])\n",
    "### equivalent;\n",
    "assert torch.all(x[:,:2] == x[:,:2,:])\n",
    "y = torch.randint(0, 10, (2,3,4))\n",
    "print(y)\n",
    "y[None, :, :].shape # adds a dummy dimension at dim=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" change tensor data type \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (20,))\n",
    "print(x.dtype)\n",
    "x = x.float()\n",
    "print(x.dtype)\n",
    "x = x.double()\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_note_:\n",
    "* if x.requires_grad=True, can not call numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use torch.tensor.item() to return single-element tensor as a python number \"\"\"\n",
    "\n",
    "x = torch.tensor([1.0])\n",
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" return size of a tensor \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randn(5,3)\n",
    "# return a torch.Size object of dimensions along each axis\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "# return number of axis\n",
    "print(x.dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" torch.sum(): sum along a tensor dimension \"\"\"\n",
    "\n",
    "x = torch.randn(5,3,4)\n",
    "x1 = torch.sum(x,dim=1,keepdim=False)   # keepdim=False is the default behavior\n",
    "print(x1.shape)\n",
    "x2 = torch.sum(x,dim=1,keepdim=True)\n",
    "print(x2.shape)\n",
    "print(torch.sum(x).shape)     # just sums all elements in x, returns a scalar tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" detach() \"\"\"\n",
    "\n",
    "x = torch.randn(5, 3, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.sum(x * 2)\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "\n",
    "# create z by detach() x from compute graph\n",
    "z = y.detach()\n",
    "print(y.requires_grad)          # x is un-modified\n",
    "print(z.requires_grad)          # z.requires_grad set to False\n",
    "\n",
    "# call backward() method on y, with requires_grad=True\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# z and y shares same storage, any change to z will update y\n",
    "print(y)\n",
    "print(z)\n",
    "z += 1\n",
    "print(z)\n",
    "print(y)                        # y is also updated to +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" apply element-wise transformations to tensor \"\"\"\n",
    "\n",
    "x = torch.randn(5, 3)\n",
    "print(x)\n",
    "\n",
    "# take exp\n",
    "x = torch.exp(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_note_:\n",
    "* Pytorch does not currently support custom element-wise lambda functions for tensor\n",
    "  * a solution maybe to convert to np.ndarrays first, or use a stack of built-in element-wise functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" conditional slicing \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.rand(32,10)                               # x: a 32x10 tensor (e.g., xent outputs where batch=32, num of classes = 10)\n",
    "labels = torch.randint(0,10,(32,))                  # labels: a 32x1 tensor of integers (e.g., each element is a correct label index)\n",
    "print(x)\n",
    "print(labels)\n",
    "\n",
    "# torch.max(tensor, dim) will return a tuple of two tensors (val_max, arg_max)\n",
    "max_val, indices = torch.max(x, dim=1)\n",
    "# tensor[condition] (for 1d tensor; for higher d, use slicing syntax like [:, condition]) slices the tensor if the condition is evaluated to be True element-wise\n",
    "torch.sum(max_val[indices == labels]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" masked slicing conditioned on another tensor \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.zeros(32,10)                              # x: a 32x10 tensor (e.g., xent outputs where batch=32, num of classes = 10)\n",
    "labels = torch.randint(0,10,(32,))                  # labels: a 32x1 tensor of integers (e.g., each element is a correct label index)\n",
    "\n",
    "# construct a 32x10 mask tensor obj, mask[i][j] = True if labels[i] == j -> use values in labels as indices\n",
    "# note that gather(), select() methods all broadcast indices along axis other than dim specified, so won't work here\n",
    "mask = torch.BoolTensor([[True if i == labels[j] else False for i in range(x.size()[1]) ] for j in range(x.size()[0])])\n",
    "torch.masked_select(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.index_select(tensor, dim, indices) \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 9, (3, 4))\n",
    "print(x)\n",
    "indices = torch.tensor([0, 2])\n",
    "print(torch.index_select(x, 1, indices))\n",
    "print(torch.index_select(x, 0, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_note_:\n",
    "* mask must be torch.BoolTensor type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" check if two tensors are equal \"\"\"\n",
    "\n",
    "# torch.equal() returns True if all elements are equal\n",
    "x = torch.rand(3,3)\n",
    "y = torch.rand(3,3)\n",
    "\n",
    "# returns a final boolean with torch.equal()\n",
    "print(torch.equal(x,y))\n",
    "# returns a BoolTensor with torch.eq()\n",
    "print(torch.eq(x,y))\n",
    "# or\n",
    "print(torch.all(torch.eq(x,y)))\n",
    "\n",
    "z = x.clone()\n",
    "print(torch.equal(x,z))\n",
    "\n",
    "# to return True if no elements are equal, can do the following\n",
    "print((x != y).all())\n",
    "\n",
    "# to return True if at least some elements are not equal, can do the following\n",
    "print((x != y).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" return all zero elements' indices in a tensor \"\"\"\n",
    "\n",
    "x = torch.rand(3,3)\n",
    "y = torch.rand(3,3)\n",
    "y[:,2] = x[:,2]\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# return the non-zero element indices as a tuple\n",
    "tup = (x - y).nonzero(as_tuple=True)\n",
    "print(tup)\n",
    "# can directly use the returned tuple to access the tensor elements\n",
    "print(x[tup])\n",
    "\n",
    "# return the zero element indices as a tuple\n",
    "tup2 = ((x - y) == 0).nonzero(as_tuple=True)\n",
    "print(tup2)\n",
    "print(x[tup2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.stack() vs torch.cat()\n",
    "* stack() a list of tensors along a new axis, output tensor would have an additional axis than input tensors\n",
    "* cat() a list of tensors along an existing axis, output tensor has equal # of axes as input tensors\n",
    "* stack() = unsqueeze() + cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" torch.stack() + torch.transpose() + torch.flatten() \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "t = torch.rand(1, 9, 1, 1)\n",
    "print(t)\n",
    "print(t.shape)\n",
    "lst = torch.split(t, 3, dim=1)\n",
    "print(lst)\n",
    "a = torch.cat(lst, dim=1)\n",
    "print('break')\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "x = torch.stack(lst, dim=1)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "\n",
    "y = torch.transpose(x, dim0=1, dim1=2)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "z = torch.flatten(y, start_dim=1, end_dim=2)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 10, (12,))\n",
    "print(x)\n",
    "y = x.numpy()\n",
    "print(y)\n",
    "import numpy as np\n",
    "np.array_split(y, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.stack() adds a new dimension \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x1 = torch.rand(1,)\n",
    "x2 = torch.rand(1,)\n",
    "x3 = torch.rand(1,)\n",
    "lst = [x1, x2, x3]\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "y1 = torch.stack(lst, dim=0)    # adds the new dimension at dim=0 (becomes first axis)\n",
    "print(y1.shape)\n",
    "print(y1)\n",
    "y2 = torch.stack(lst, dim=1)    # adds the new dimension at dim=1 (becomes second & last axis)\n",
    "print(y2.shape)\n",
    "print(y2)\n",
    "y3 = torch.stack(lst, dim=-1)   # adds the new dimension at dim=1 (becomes second & last axis)\n",
    "print(y3.shape)\n",
    "print(y3)\n",
    "y4 = torch.stack(lst, dim=-2)   # adds the new dimension at dim=0 (becomes first axis)\n",
    "print(y4.shape)\n",
    "print(y4)\n",
    "# y5 = torch.stack(lst, dim=2)    # error: dim must be in range [-(x.dim()+1), x.dim()], so in this case x.dim()==1 and the range is [-2, 1]\n",
    "# the rule is very simple:\n",
    "# - `dim` must be in the above range\n",
    "# - the new axis is added at `dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" keep in mind the difference between tensor([1]), tensor(1), tensor(1.)\"\"\"\n",
    "\n",
    "x1 = torch.tensor([1,2,3])\n",
    "x2 = torch.tensor([1])\n",
    "x3 = torch.tensor(1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x2.dim())\n",
    "print(x3.dim())\n",
    "# float\n",
    "x4 = torch.tensor(1.)\n",
    "print(x4)\n",
    "print(x4.shape)\n",
    "x5 = torch.tensor(1)\n",
    "print(x5)\n",
    "print(x5.reshape((1,)))\n",
    "x3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "lst = [torch.tensor(1), torch.tensor(1), torch.tensor(1)]\n",
    "torch.cat(lst, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" element-wise tensor multiplication by broadcasting \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (5, 3, 3))\n",
    "y = torch.randint(0, 10, (5, 1, 1))\n",
    "print(x)\n",
    "print(y)\n",
    "# can directly multiply two tensors, if one of the axes matches in dimensions\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "torch.repeat_interleave(x, repeat, dim)\n",
    "\n",
    "- repeats every entry in tensor x by number=repeat for that dimension\n",
    "- repeat must have the same size as input along dim\n",
    "- dim is optional; if not specified, repeat must be integer\n",
    "\"\"\"\n",
    "\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])\n",
    "print(x)\n",
    "\n",
    "# repeat all entries in all dimensions & return a flattened tensor\n",
    "x_1 = torch.repeat_interleave(x, 2)\n",
    "print(x_1)\n",
    "\n",
    "# repeat entry x[i] by repeat[i] along dim=0\n",
    "x_2 = torch.repeat_interleave(x, torch.tensor([1,2,3,4]), dim=0)\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create a random tensors for specified shape \"\"\"\n",
    "\n",
    "# integers; each entry within range [0, 9]; shape = (2,2,4)\n",
    "x = torch.randint(0, 10 ,(2, 2, 4))\n",
    "\n",
    "# random floats of shape torch.tensor([5,3,4]) sampled from N(0, 1)\n",
    "y_n = torch.randn(5, 3, 4)\n",
    "# random floats sampled from [0, 1] uniformly\n",
    "y_p = torch.rand(5, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" return data type in tensor \"\"\"\n",
    "import torch\n",
    "x = torch.randn(5, 3)\n",
    "len(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "enumerate a tensor \n",
    "\n",
    "- equivalent to split a tensor along dim=0\n",
    "\"\"\"\n",
    "\n",
    "x = torch.randn(5, 3, 4)\n",
    "print(x)\n",
    "\n",
    "for count, matrix in enumerate(x):\n",
    "    print(count)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.bmm(input1, input2)\n",
    "\n",
    "- batch matrix-matrix product\n",
    "- input1=bxnxm, input2=bxmxp; returns bxnxp\n",
    "\"\"\"\n",
    "\n",
    "torch.bmm(torch.ones(2, 1, 3), torch.ones(2, 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.arange(start=0, stop, step=1)\n",
    "\n",
    "- generate a 1-D tensor of arithmetic sequence\n",
    "\n",
    "note:\n",
    "- start=0, step=1 as defaults, these two arguments are optional\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# start=1, end=10, step=2\n",
    "x = torch.arange(1, 10, 2)\n",
    "print(x)\n",
    "# start=1, end=10, step=1\n",
    "y = torch.arange(10)\n",
    "print(y)\n",
    "z = torch.randint(0, 10, (2,3))\n",
    "print(z)\n",
    "print(torch.arange(z.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" product of all elements in tensor \"\"\"\n",
    "\n",
    "x = torch.randint(0, 10, (2, 2))\n",
    "print(x)\n",
    "print(torch.prod(x, dim=0))\n",
    "print(torch.prod(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" torch.repeat \"\"\"\n",
    "import torch\n",
    "x = torch.randint(0, 10, (2, 2))\n",
    "print(x)\n",
    "# repeat 2 times along dimension=0 and 3 times along dimension=1;\n",
    "x.repeat(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.clamp clips all elements in input tensor by a range \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (3, 4, 5))\n",
    "print(x)\n",
    "y = torch.clamp(x, min=3, max=6)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* convert from numpy.dtype to torch.dtype is problematic\n",
    "* see [this post](https://discuss.pytorch.org/t/converting-a-numpy-dtype-to-torch-dtype/52279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.tensor vs torch.from_numpy() \"\"\"\n",
    "\n",
    "# from_numpy() expects a np.ndarray, so can not work for scalars\n",
    "# otherwise two method yields identical tensors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.BoolTensor requires a list to instantiate \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.BoolTensor([True for _ in range(10)])\n",
    "print(x)\n",
    "\n",
    "y = torch.BoolTensor((3,))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use einops \"\"\"\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "data, labels = batch\n",
    "print(data.shape)\n",
    "a = rearrange(data[0:1, :3], 'c h w -> h w c')\n",
    "print(a.shape)\n",
    "b = rearrange(a, 'h w c -> () c h w')\n",
    "print(b.shape)\n",
    "c = rearrange(b, 'b c h w -> b (c h w)')\n",
    "print(c.shape)\n",
    "d = rearrange(c, '() classes -> classes')\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.unsqueeze() \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1,2,3,4])\n",
    "print(x.shape)\n",
    "x1 = x.unsqueeze(0)\n",
    "print(x1)\n",
    "print(x1.shape)\n",
    "x2 = x.unsqueeze(1)\n",
    "print(x2)\n",
    "print(x2.shape)\n",
    "x3 = x.unsqueeze(-1)\n",
    "print(x3)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([0,1,0,1])\n",
    "print(x.dim())\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" some special tensor methods \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.rand((2,2))\n",
    "print(x)\n",
    "\n",
    "### returns the sign of a tensor\n",
    "print(x.sign())\n",
    "### matrix transpose (dim <= 2)\n",
    "print(x.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" element-wise operators broadcast \"\"\"\n",
    "import torch\n",
    "import einops\n",
    "\n",
    "# broadcast rules:\n",
    "# 1. tensors match in .dim(); exact match in .shape not needed;\n",
    "# 2. tensors match exactly in .shape along non-singleton dimenesions, i.e., dimensions w/t elements >= 1;\n",
    "# so if need to apply masking to a specific dimension, the mask tensor must:\n",
    "# 1) has the same .dim() as x; 2) match in .shape w/t x at the specific axis; 3) all other dimension should be singleton dimensions\n",
    "\n",
    "x = torch.randint(0, 9, (2, 3, 4))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "mask = torch.tensor([0,1,0])\n",
    "mask1 = mask.unsqueeze(0).unsqueeze(-1)\n",
    "print(mask1)\n",
    "print(mask1.shape)\n",
    "print(x * mask1)\n",
    "# use einops\n",
    "mask2 = einops.repeat(mask, 'h -> n1 h n2', n1=1, n2=1)\n",
    "print(mask2)\n",
    "print(x * mask2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.norm: now deprecated; use torch.linalg.norm instead \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" check all elements in a tensor satisfies a condition or not \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (2,3,4))\n",
    "print(x)\n",
    "print(torch.all(x >= 0))\n",
    "\n",
    "y = torch.randn((2,3,4))\n",
    "print(y)\n",
    "print(torch.all(y >= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" produce an identity matrix \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.ones(3)\n",
    "torch.diag(x)\n",
    "\n",
    "### a better way is to use torch.eye()\n",
    "torch.eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.unbind() \"\"\"\n",
    "\n",
    "# returns a tuple of tensors s/t the dimension of dim is removed;\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(x)\n",
    "print(x.unbind(dim=1))\n",
    "\n",
    "y = torch.randint(0, 10, (2,3,4))\n",
    "print(y.shape)\n",
    "from einops import rearrange\n",
    "y1 = rearrange(y, '... (d j) -> ... d j', j=2)\n",
    "print(y1.shape)\n",
    "print(y)\n",
    "print(y1)\n",
    "z1, z2 = y1.unbind(dim=-1)\n",
    "print(z1)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.expand \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (5,))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "y = x.expand((2, 5))\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "### expand(1,-1) has the same effect of adding a dummy dimension at shape[0];\n",
    "z = x.expand((1, -1))\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1,2,3,4)\n",
    "print(x)\n",
    "print(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.tensor([[1,2],[3,4]]).numpy()\n",
    "y = torch.tensor([5,6,7]).numpy()\n",
    "\n",
    "z = np.multiply.outer(x, y)\n",
    "z = torch.from_numpy(z)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randint(0, 10, (1, 2, 5, 10))\n",
    "y = torch.randint(0, 10, (1, 2, 3, 10))\n",
    "\n",
    "a = (2 ** 2)\n",
    "print(a)\n",
    "\n",
    "print((x * a))\n",
    "\n",
    "torch.einsum('...ik, ...jk -> ...ij', (x * 2), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tensor slicing: alternate odd / even positions. \"\"\"\n",
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "x = torch.randint(0, 10, (4, 6))\n",
    "print(x)\n",
    "a = x[:, 0::2]      # even  \n",
    "b = x[:, 1::2]      # odd\n",
    "print(a)\n",
    "print(b)\n",
    "print(x.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" several ways to interleave two tensors. \"\"\"\n",
    "import torch\n",
    "\n",
    "a = torch.ones([2, 3, 4])\n",
    "b = torch.zeros([2, 3, 4])\n",
    "print(a.shape)\n",
    "# interleave along dim = 1\n",
    "x = map(lambda t: torch.stack(t, dim=2),  zip(a.unbind(dim=1), b.unbind(dim=1)))\n",
    "print(torch.cat(list(x), dim=2))\n",
    "# interleave along dim = 2\n",
    "x = map(lambda t: torch.stack(t, dim=1),  zip(a.unbind(dim=2), b.unbind(dim=2)))\n",
    "print(torch.cat(list(x), dim=1))\n",
    "\n",
    "# there is probably no need to do unbind, can just stack directly then view;\n",
    "a = torch.ones([3, 4])\n",
    "b = torch.zeros([3, 4])\n",
    "print(torch.stack((a, b), dim=2).view(3, 8))\n",
    "print(torch.stack((a, b), dim=1).view(6, 4))\n",
    "\n",
    "# or simply use rearrange; note that to interleave must always put `t` in the last within the brackets, e.g., (w t), not (t w)\n",
    "from einops import rearrange\n",
    "a = torch.ones([3, 4])\n",
    "b = torch.zeros([3, 4])\n",
    "print(rearrange([a, b], 't h w -> h (w t)'))\n",
    "print(rearrange([a, b], 't h w -> (h t) w'))\n",
    "# how about more dimensions?\n",
    "a = torch.ones([2, 3, 4])\n",
    "b = torch.zeros([2, 3, 4])\n",
    "print(rearrange([a, b], 't c h w -> c h (w t)'))    # interleave along dim = 2, shape = [2,3,8]\n",
    "print(rearrange([a, b], 't c h w -> c (h t) w'))    # interleave along dim = 1, shape = [2,6,4]\n",
    "print(rearrange([a, b], 't c h w -> (c t) h w'))    # interleave along dim = 0, shape = [4,3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" concat two tensors along dim then interleave them along the same dimension. \"\"\"\n",
    "import torch\n",
    "\n",
    "a = torch.ones([2, 3])\n",
    "b = torch.zeros([2, 3])\n",
    "torch.stack((a, b), dim=-1).view(2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" .contiguous() \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 9, (2, 3, 4))\n",
    "x1 = x.contiguous()\n",
    "\n",
    "# see this post: https://stackoverflow.com/questions/48915810/pytorch-what-does-contiguous-do\n",
    "# .contiguous() just makes a copy of the tensor that has the same memory layout as the tensor's shape\n",
    "# normally not needed to call explicitly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" torch.ones_like() \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randint(0, 9, (2, 3, 4))\n",
    "print(x.shape)\n",
    "y = torch.ones_like(x)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "# torch.ones_like() is equivalent to:\n",
    "z = torch.ones(x.size(), dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 3\n",
    "seq_len = 10\n",
    "num_heads = 5\n",
    "\n",
    "mask = torch.ones((batch_size, seq_len))\n",
    "print(mask.shape)\n",
    "mask = mask.int()[:, None, :].repeat(1, num_heads, 1).reshape(batch_size * num_heads, seq_len)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" use tensor slicing to reshape, reduce or broadcast tensors \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.ones((10, 3, 5, 12))\n",
    "print(x.shape)\n",
    "x1 = x[:, 0, 0, :]\n",
    "print(x1.shape)\n",
    "x2 = x[:, 0, 1, :]      # if a dim is a single integer, that dimension is reduced in returned tensor\n",
    "print(x2.shape)\n",
    "x3 = x2[:, None, :]     # use an added dimension w/t `None` to add a dummy dimension\n",
    "print(x3.shape)\n",
    "x4 = x[:, 0, 0:1, :]    # or use i:j slicing notation to avoid reducing that dimension\n",
    "print(x4.shape)\n",
    "\n",
    "y = torch.ones(2, 3)\n",
    "print(y.shape)\n",
    "y1 = y[:,:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tensor > 0 \"\"\"\n",
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "x1 = x > 0\n",
    "print(x1)\n",
    "print(x1.int())         # convert False->0, True->1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" convert a list of numbers with torch.tensor() \"\"\"\n",
    "# note: lower-case `t`, not torch.Tensor() !\n",
    "import torch\n",
    "\n",
    "x = [[1, 2, 3], [4, 5,6], [7,8,9]]\n",
    "x = [torch.tensor(b, dtype=torch.int32) for b in x]\n",
    "print(x)\n",
    "torch.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" stack a list of tensors with padding to the max length of the tensors. \"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c04e14e757fcd5c931b20c230f107ce3be1b6ffeb36695f3d01a868d65a6b9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
