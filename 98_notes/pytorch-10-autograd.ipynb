{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c04e14e757fcd5c931b20c230f107ce3be1b6ffeb36695f3d01a868d65a6b9cc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### leaf nodes:\n",
    "* a PyTorch leaf node is a tensor which is not created by any operation tracked by the autograd engine\n",
    "* leaf nodes:\n",
    "    * store gradients\n",
    "    * are usually the inputs or weights to the forward graph\n",
    "    * are not created by operations that can be traced back to any tensor that has requires_grad=True\n",
    "* the other type of nodes in a PyTorch compute graph is the intermediate nodes:\n",
    "    * have a grad_fn field that points to a node in the backward graph\n",
    "    * do not store gradients by default, unless register a hook called retain_grad()\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" leaf nodes & intermediate nodes \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "# leaf node A\r\n",
    "a = torch.tensor(1.0, requires_grad=True)\r\n",
    "# leaf node B\r\n",
    "b = torch.tensor(2.0, requires_grad=True)\r\n",
    "# intermediate node C\r\n",
    "c = a*b\r\n",
    "c.backward()\r\n",
    "print(c.grad)\r\n",
    "print(a.grad)\r\n",
    "print(b.grad)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" autograd on a tensor \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "\r\n",
    "x1 = torch.randn((2, 4), requires_grad=True)\r\n",
    "x2 = torch.randn((2, 4), requires_grad=True)\r\n",
    "y = x1 + x2\r\n",
    "y.backward(y)\r\n",
    "print(x1.grad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\" autograd on the output tensor of a layer. \"\"\"\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "model = nn.Sequential(\r\n",
    "    nn.Linear(10, 20),\r\n",
    ")\r\n",
    "x = torch.randn((2, 10), requires_grad=True)\r\n",
    "out = model(x)\r\n",
    "out.backward(out)\r\n",
    "print(x.grad)\r\n",
    "print(out.grad)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}