{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### leaf nodes:\n",
    "* a PyTorch leaf node is a tensor which is not created by any operation tracked by the autograd engine\n",
    "* leaf nodes:\n",
    "    * store gradients\n",
    "    * are usually the inputs or weights to the forward graph\n",
    "    * are not created by operations that can be traced back to any tensor that has requires_grad=True\n",
    "* the other type of nodes in a PyTorch compute graph is the intermediate nodes:\n",
    "    * have a grad_fn field that points to a node in the backward graph\n",
    "    * do not store gradients by default, unless register a hook called retain_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" leaf nodes & intermediate nodes \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# leaf node A\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "# leaf node B\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "# intermediate node C\n",
    "c = a*b\n",
    "c.backward()\n",
    "print(c.grad)\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" autograd on a tensor \"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "x1 = torch.randn((2, 4), requires_grad=True)\n",
    "x2 = torch.randn((2, 4), requires_grad=True)\n",
    "y = x1 + x2\n",
    "y.backward(y)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" autograd on the output tensor of a layer. \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    ")\n",
    "x = torch.randn((2, 10), requires_grad=True)\n",
    "out = model(x)\n",
    "out.backward(out)\n",
    "print(x.grad)\n",
    "print(out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" .retain_grad() vs requires_grad=True \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    ")\n",
    "\n",
    "# 1. gradients are not populated for both leaf tensor x and intermediate tensor out\n",
    "x = torch.randn((2, 10))\n",
    "out = model(x)\n",
    "out.backward(out)\n",
    "print(x.grad)       # None\n",
    "print(out.grad)     # None\n",
    "\n",
    "# 2. gradients are populated for intermediate tensor out by declaring out.retain_grad() before backward-pass\n",
    "x1 = torch.randn((2, 10))\n",
    "out1 = model(x1)\n",
    "out1.retain_grad()\n",
    "# out.requires_grad=True -> error: can only change requires_grad flag for leaf nodes\n",
    "# x1.retain_grad() -> error: can't retain_grad() on a tensor that has requires_grad=False\n",
    "out1.backward(out1)\n",
    "print(x1.grad)      # None\n",
    "print(out1.grad)\n",
    "\n",
    "# 3. gradients are populated for leaf node x by declaring requires_grad=True\n",
    "x2 = torch.randn((2, 10), requires_grad=True)\n",
    "out2 = model(x2)\n",
    "out2.backward(out2)\n",
    "print(x2.grad)  \n",
    "print(out2.grad)    # None\n",
    "\n",
    "# model parameters (weights and biases) by default have requires_grad=True\n",
    "# access all (named) parameters in model in the following way:\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n",
    "    print(param.requires_grad)  # all True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .retain_grad() vs requires_grad=True\n",
    "* .retain_grad()\n",
    "    * usually used by intermediate tensors (e.g. outputs);\n",
    "    * if use it for a leaf tensor (e.g. input), the `requires_grad=True` must be set first for that tensor, but in this case the effects are redundant;\n",
    "* requires_grad=True\n",
    "    * usually set for leaf tensors;\n",
    "    * default = True; see the [docs](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html) for the underlying method `requires_grad_`;\n",
    "        * Q: but why in the previous example `x` doesn't by default has gradients populated?\n",
    "            * maybe b/c be default input tensors, although are leaf nodes, do not usually need gradients for training -> only weights / biases need gradients;\n",
    "* usually in my model, inputs and weights are leaf tensors that have `requires_grad=True` set, so their gradients are populated when computed;\n",
    "* the outputs & hidden_states / activatations are intermediate tensors; their gradients are not stored by default, instead they have a field called `grad_fn` that points to the proper node in the backward graph so that backprop can be handled;\n",
    "    * this mechanism is probably used to same mem footprint, as really only parameter gradients are needed for training;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c04e14e757fcd5c931b20c230f107ce3be1b6ffeb36695f3d01a868d65a6b9cc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
