{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on pytorch data loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load cifar-10 dataset \"\"\" \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# load train set\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader_cifar10 = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# load test set\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
    "testloader_cifar10 = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" select a random data point from trainset \"\"\"\n",
    "\n",
    "# perm is a tensor of same size as trainset.data, but a randomized permutation of indices\n",
    "perm = torch.randperm(len(trainset.data))\n",
    "torch.Tensor(trainset.targets)[perm][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" select a random batch from dataloader \"\"\"\n",
    "\n",
    "# the sample returned is random b.c. shuffle=True in DataLoader call\n",
    "# iter(trainloader).__iter__().next() is equivalent to iter(trainloader).next() -> returns a list of two tensors ([0]:images, [1]:labels)\n",
    "images, labels = iter(trainloader_cifar10).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the difference of the two method:\n",
    "\n",
    "* using trainset.data would return the raw data points without transformations (although transform is applied in CIFAR10 call, it seems only executed by dataloader class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    MNIST is a Dataset object, can be accessed by __getitem__ method\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader_mnist = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "torchvision.utils.save_image(tensor=trainset.__getitem__(0)[0], fp='./mnist_sample.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" convert dataloder obj to an iteratable obj \"\"\"\n",
    "iterable_cifar10 = iter(trainloader_cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" iterate over a dataloader \"\"\"\n",
    "\n",
    "for i in range(5):\n",
    "    images, labels = iterable_cifar10.next()\n",
    "    print(labels)       # note that labels are distinct for each iteration in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create DataLoader object from tensors \"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader([torch.zeros(3,3,3) for idx in range(3)])"
   ]
  },
  {
   "source": [
    "### DataLoader(dataset, ...)\n",
    "* takes only one mandatory argument, dataset\n",
    "* dataset can be either a map-style or an iterator-style object\n",
    "    * see [pytorch docs](https://pytorch.org/docs/stable/data.html#map-style-datasets) for details\n",
    "    * in practice:\n",
    "        * a map-style object is one that can be accessed by dataset[idx]\n",
    "        * an iterator style object is one that can be access by next(iter(dataset))\n",
    "    * so can use any python built-in objects or custom class objects as long as the corresponding functions (i.e., __getitem__() for map-style & __iter__() for iterator-style)\n",
    "      are implemented properly\n",
    "        * e.g., use a list would be a good map-style example\n",
    "* with this mechanism, any trainer function can be defaulted to always accept a DataLoader object as argument\n",
    "    * in practice, can feed to this argument any type of DataLoader that is constructed from a standard dataset, custom dataset, or even small tensors, np arrays for testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dl = DataLoader([torch.zeros(3,3,3) for idx in range(10)])\n",
    "\n",
    "for idx, (i, batch) in zip([e for e in range(5)], enumerate(dl)):\n",
    "    print(idx)\n",
    "    print(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" use the regular enumerate(dataloader) syntax but only train for a fraction of an epoch \"\"\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# load train set\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "for idx, (i, (batch, labels)) in zip([e for e in range(5)], enumerate(trainloader)):\n",
    "    print(idx)\n",
    "    print(batch)"
   ]
  },
  {
   "source": [
    "* always place iter(dataloader) outside for loops for better performance\n",
    "* see [this post](https://github.com/pytorch/pytorch/issues/1917#issuecomment-433698337) and [this post](https://stackoverflow.com/questions/53280967/pytorch-nextitertraining-loader-extremely-slow-simple-data-cant-num-worke) for details"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" get one batch from a standard dataset's dataloader & make it into a DataLoader object that iterates over the same batch \"\"\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# load train set\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "batch = next(iter(trainloader))\n",
    "batch_dl = DataLoader([batch for _ in range(3)])\n",
    "batch_iter = iter(batch_dl)\n",
    "for idx in range(3):\n",
    "    data, labels = next(batch_iter)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" get a random sub-set of samples from a single batch & return as a new iterator over this sampled batch \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# load train set\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "batch = next(iter(trainloader))\n",
    "\n",
    "data, labels = batch\n",
    "\n",
    "arr = np.zeros(4, dtype=bool)\n",
    "arr[:3] = 1\n",
    "np.random.shuffle(arr)\n",
    "arr.tolist()\n",
    "data[arr, :3].shape\n",
    "\n",
    "tup = (data[arr], labels[arr])\n",
    "dl = DataLoader([tup for _ in range(5)], batch_size=None)   # set batch_size=None to disable automatic batching (by default an extra batch dim is added)\n",
    "sampled_batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}